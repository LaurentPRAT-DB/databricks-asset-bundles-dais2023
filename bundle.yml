# yaml-language-server: $schema=bundle_config_schema.json
bundle:
  name: medium_post_report

# By default we load files matching these patterns:
#
#   include:
#     - "*.yml"
#     - "*/*.yml"
#

workspace:
  host: https://e2-dogfood.staging.cloud.databricks.com/

resources:
  pipelines:
    fe_medium_metrics:
      name: "[${bundle.environment}] Metrics for FE Medium posts"
      target: "medium_post_report_${bundle.environment}"
      libraries:
        - file:
            path: ./ingest.py
        - file:
            path: ./get_metrics.py

      channel: preview

      configuration:
        "bundle.file_path": "/Workspace/${workspace.file_path}"
  jobs:
    fe_medium_metrics:
      name: "[${bundle.environment}] Metrics for FE Medium Posts"
      tasks:
        - task_key: dlt_medium_pipeline
          pipeline_task:
            pipeline_id: ${resources.pipelines.nyc_taxi_pipeline.id}
        - task_key: medium_notebook_report
          notebook_task:
            base_parameters:
              dbname: "medium_post_report_${bundle.environment}"
            notebook_path: ./shark_sightings.py
          new_cluster:
            spark_version: 10.4.x-scala2.12
            num_workers: 1
            node_type_id: i3.xlarge  


environments:
  development:
    default: true

    resources:
      pipelines:
        fe_medium_metrics:
          development: true


  qa:
    # This environment is when deploying from a pull request on GitHub.
    resources:
      pipelines:
        fe_medium_metrics:
          development: true


  production:
    # We can configure our production workspace below if we have to.
    #
    # workspace:
    #   host: https://my-production-workspace.cloud.databricks.com/
    #

    resources:
      pipelines:
        fe_medium_metrics:
          development: false

          photon: true

          clusters:
            - autoscale:
                min_workers: 2
                max_workers: 8
